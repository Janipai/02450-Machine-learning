#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Tue Sep 12 19:00:39 2023

@author: shania
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Load the seeds csv data using the Pandas library
filename = '/home/shania/02450/seeds/seeds_dataset.txt'
df = pd.read_csv(filename, delimiter=r'\s+')

raw_data = df.values  
#%%
cols = range(0, 7) 
X = raw_data[:, cols]

attributeNames = np.asarray(df.columns[cols])
classLabels = raw_data[:,-1] 
classNames = np.unique(classLabels)

classDict = dict(zip(classNames,range(len(classNames))))
y = np.array([classDict[cl] for cl in classLabels])

N, M = X.shape
C = len(classNames)
# %%
#####################################################33
# Boxplot

plt.figure(figsize=(10, 8))
sns.boxplot(data=df, orient='vertical')
plt.title('Boxplots for wheat seeds')
plt.xticks(rotation=90)  # Rotate x-axis labels if needed
plt.show()
#%%
#####################################################33
# Boxplot
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['C'], orient='vertical')
plt.title('Boxplot for Compactness')
plt.show()
#%%
plt.figure(figsize=(8, 6))
sns.boxplot(x=df['AC'], orient='vertical')
plt.title('Boxplot for AsymmetryCoefficient')
plt.show()
#%%
#####################################################33
# Histogram
plt.figure(figsize=(10, 8))
sns.pairplot(df, diag_kind='hist', hue='T', palette=sns.color_palette(['red', 'blue', 'green']))
plt.suptitle('Histogram Matrix')
plt.show()
# %%
#####################################################33
# Clasification

X_c = X.copy();
y_c = y.copy();
attributeNames_c = attributeNames.copy();
i = 3; j = 4;
color = ['r','g', 'b']
plt.title('seed classification problem')
for c in range(len(classNames)):
    idx = y_c == c
    plt.scatter(x=X_c[idx, i],
                y=X_c[idx, j], 
                c=color[c], 
                s=50, alpha=0.5,
                label=classNames[c])
plt.legend()
plt.xlabel(attributeNames_c[i])
plt.ylabel(attributeNames_c[j])
plt.show()
# %%
#####################################################33
# Regression
data = np.concatenate((X_c, np.expand_dims(y_c,axis=1)), axis=1)

y_r = data[:, 2]
X_r = data[:, [0, 1, 3, 4]]

species = np.array(X_r[:, -1], dtype=int).T
K = species.max()+1
species_encoding = np.zeros((species.size, K))
species_encoding[np.arange(species.size), species] = 1

X_r = np.concatenate( (X_r[:, :-1], species_encoding), axis=1) 

targetName_r = attributeNames_c[2]
attributeNames_r = np.concatenate((attributeNames_c[[0, 1, 3]], classNames), 
                                  axis=0)

N,M = X_r.shape

i = 2  
plt.title('Seed regression problem')
plt.plot(X_r[:, i], y_r, 'o')
plt.xlabel(attributeNames_r[i]);
plt.ylabel(targetName_r);
# %%
##################################################
# missing data
missing_values = df.isnull()
missing_count = missing_values.sum()


# Create a heatmap of missing values
plt.figure(figsize=(10, 6))
sns.heatmap(missing_values, cbar=False, cmap='viridis')
plt.title('Missing Values Heatmap')
plt.show()

stat = df.describe()
# %%
#Excersie 2
from matplotlib.pyplot import figure, plot, title, legend, xlabel, ylabel, show

# Data attributes to be plotted
i = 0
j = 2

plot(X[:, i], X[:, j], 'o')

f = figure()
title('NanoNose data')

for c in range(C):
    # select indices belonging to class c:
    class_mask = y==c
    plot(X[class_mask,i], X[class_mask,j], 'o',alpha=.3)

legend(classNames)
xlabel(attributeNames[i])
ylabel(attributeNames[j])

# Output result to screen
show()
#%%
import matplotlib.pyplot as plt
from scipy.linalg import svd

# Subtract mean value from data
Y = X - np.ones((N,1))*X.mean(axis=0)

# PCA by computing SVD of Y
U,S,V = svd(Y,full_matrices=False)

# Compute variance explained by principal components
rho = (S*S) / (S*S).sum() 

threshold = 0.9

# Plot variance explained
plt.figure()
plt.plot(range(1,len(rho)+1),rho,'x-')
plt.plot(range(1,len(rho)+1),np.cumsum(rho),'o-')
plt.plot([1,len(rho)],[threshold, threshold],'k--')
plt.title('Variance explained by principal components');
plt.xlabel('Principal component');
plt.ylabel('Variance explained');
plt.legend(['Individual','Cumulative','Threshold'])
plt.grid()
plt.show()
#%%
from matplotlib.pyplot import figure, plot, title, xlabel, ylabel, show, legend
from scipy.linalg import svd

# Subtract mean value from data
Y = X - np.ones((N,1))*X.mean(0)

# PCA by computing SVD of Y
U,S,Vh = svd(Y,full_matrices=False)
# scipy.linalg.svd returns "Vh", which is the Hermitian (transpose)
# of the vector V. So, for us to obtain the correct V, we transpose:
V = Vh.T    

# Project the centered data onto principal component space
Z = Y @ V

# Indices of the principal components to be plotted
i = 0
j = 1

# Plot PCA of the data
f = figure()
title('NanoNose data: PCA')
#Z = array(Z)
for c in range(C):
    # select indices belonging to class c:
    class_mask = y==c
    plot(Z[class_mask,i], Z[class_mask,j], 'o', alpha=.5)
legend(classNames)
xlabel('PC{0}'.format(i+1))
ylabel('PC{0}'.format(j+1))

# Output result to screen
show()
#%%
import matplotlib.pyplot as plt
from scipy.linalg import svd

Y = X - np.ones((N,1))*X.mean(0)
U,S,Vh = svd(Y,full_matrices=False)
V=Vh.T
N,M = X.shape

pcs = [0,1,2]
legendStrs = ['PC'+str(e+1) for e in pcs]
c = ['r','g','b']
bw = .2
r = np.arange(1,M+1)
for i in pcs:    
    plt.bar(r+i*bw, V[:,i], width=bw)
plt.xticks(r+bw, attributeNames)
plt.xlabel('Attributes')
plt.ylabel('Component coefficients')
plt.legend(legendStrs)
plt.grid()
plt.title('NanoNose: PCA Component Coefficients')
plt.show()

print('PC2:')
print(V[:,1].T)

all_water_data = Y[y==4,:]

print('First water observation')
print(all_water_data[0,:])

print('...and its projection onto PC2')
print(all_water_data[0,:]@V[:,1])
#%%
import seaborn as sns
import matplotlib.pyplot as plt

correlation_matrix = df.corr()

plt.figure(figsize=(10, 8))
sns.heatmap(correlation_matrix, annot=True)
plt.title('Correlation Matrix Heatmap')
plt.show()
#%%
## exercise 2.1.6

r = np.arange(1,X.shape[1]+1)
plt.bar(r, np.std(X,0))
plt.xticks(r, attributeNames)
plt.ylabel('Standard deviation')
plt.xlabel('Attributes')
plt.title('NanoNose: attribute standard deviations')

## Investigate how standardization affects PCA

# Try this *later* (for last), and explain the effect
#X_s = X.copy() # Make a to be "scaled" version of X
#X_s[:, 2] = 100*X_s[:, 2] # Scale/multiply attribute C with a factor 100
# Use X_s instead of X to in the script below to see the difference.
# Does it affect the two columns in the plot equally?


# Subtract the mean from the data
Y1 = X - np.ones((N, 1))*X.mean(0)

# Subtract the mean from the data and divide by the attribute standard
# deviation to obtain a standardized dataset:
Y2 = X - np.ones((N, 1))*X.mean(0)
Y2 = Y2*(1/np.std(Y2,0))
# Here were utilizing the broadcasting of a row vector to fit the dimensions 
# of Y2

# Store the two in a cell, so we can just loop over them:
Ys = [Y1, Y2]
titles = ['Zero-mean', 'Zero-mean and unit variance']
threshold = 0.9
# Choose two PCs to plot (the projection)
i = 0
j = 1

# Make the plot
plt.figure(figsize=(10,15))
plt.subplots_adjust(hspace=.4)
plt.title('NanoNose: Effect of standardization')
nrows=3
ncols=2
for k in range(2):
    # Obtain the PCA solution by calculate the SVD of either Y1 or Y2
    U,S,Vh = svd(Ys[k],full_matrices=False)
    V=Vh.T # For the direction of V to fit the convention in the course we transpose
    # For visualization purposes, we flip the directionality of the
    # principal directions such that the directions match for Y1 and Y2.
    if k==1: V = -V; U = -U; 
    
    # Compute variance explained
    rho = (S*S) / (S*S).sum() 
    
    # Compute the projection onto the principal components
    Z = U*S;
    
    # Plot projection
    plt.subplot(nrows, ncols, 1+k)
    C = len(classNames)
    for c in range(C):
        plt.plot(Z[y==c,i], Z[y==c,j], '.', alpha=.5)
    plt.xlabel('PC'+str(i+1))
    plt.xlabel('PC'+str(j+1))
    plt.title(titles[k] + '\n' + 'Projection' )
    plt.legend(classNames)
    plt.axis('equal')
    
    # Plot attribute coefficients in principal component space
    plt.subplot(nrows, ncols,  3+k)
    for att in range(V.shape[1]):
        plt.arrow(0,0, V[att,i], V[att,j])
        plt.text(V[att,i], V[att,j], attributeNames[att])
    plt.xlim([-1,1])
    plt.ylim([-1,1])
    plt.xlabel('PC'+str(i+1))
    plt.ylabel('PC'+str(j+1))
    plt.grid()
    # Add a unit circle
    plt.plot(np.cos(np.arange(0, 2*np.pi, 0.01)), 
         np.sin(np.arange(0, 2*np.pi, 0.01)));
    plt.title(titles[k] +'\n'+'Attribute coefficients')
    plt.axis('equal')
            
    # Plot cumulative variance explained
    plt.subplot(nrows, ncols,  5+k);
    plt.plot(range(1,len(rho)+1),rho,'x-')
    plt.plot(range(1,len(rho)+1),np.cumsum(rho),'o-')
    plt.plot([1,len(rho)],[threshold, threshold],'k--')
    plt.title('Variance explained by principal components');
    plt.xlabel('Principal component');
    plt.ylabel('Variance explained');
    plt.legend(['Individual','Cumulative','Threshold'])
    plt.grid()
    plt.title(titles[k]+'\n'+'Variance explained')

plt.show()
#%%
# Assuming 'classification' is the name of your classification attribute
classification_column = 'T'

# Get a list of all attribute names (excluding the classification column)
attribute_columns = [col for col in df.columns if col != classification_column]

# Define the number of bins for the histograms
num_bins = 30

# Create subplots for each attribute
fig, axes = plt.subplots(nrows=len(attribute_columns), figsize=(8, 6 * len(attribute_columns)))

for i, attribute in enumerate(attribute_columns):
    ax = axes[i]
    
    # Use Seaborn's distplot to create the histogram and distribution curve
    sns.histplot(df, x=attribute, hue=classification_column, bins=num_bins, kde=True, ax=ax)
    
    # Set plot title
    ax.set_title(f'Histogram with Distribution Curve for {attribute}')
    
    # Add legend
    ax.legend(title=classification_column)
    
    # Optionally customize other plot settings here

plt.tight_layout()
plt.show()

#%%%
for attribute in df.columns[:-1]:  # Exclude the last column (classification attribute)
    plt.figure(figsize=(8, 6))
    sns.histplot(df, x=attribute, hue='T', bins=num_bins, kde=True,palette=sns.color_palette(['red', 'blue', 'green']))
    plt.title(f'Histogram with Distribution Curve for {attribute}')
    plt.xlabel(attribute)
    plt.ylabel('Frequency')
    plt.legend(title='classification')
    plt.grid(True)
    plt.show()         
